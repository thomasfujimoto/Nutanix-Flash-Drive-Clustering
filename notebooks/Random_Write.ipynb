{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'name', 'metric', 'queue_depth', 'num_jobs', 'blocksize','unit', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'device_type', 'model', 'operating_pci_speed_gts', 'operating_pci_width', \n",
    "]\n",
    "\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=False)\n",
    "df = df[df['data_type'] == 'Random Read']\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "# Filter dataset for Sequential Write\n",
    "\n",
    "# Define numeric columns for clustering (REMOVED:  'stddev_measure', 'median_measure','min_measure','max_measure', )\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure','operating_pci_speed_gts', 'operating_pci_width', \n",
    "]\n",
    "\n",
    "df = pd.get_dummies(df, columns=['metric'])\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Write Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Applied PCA (optimally reduce features) + Outliers + Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Assume 'df' is your DataFrame\n",
    "\n",
    "# Define input columns for clustering\n",
    "metric_columns = [col for col in df.columns if col.startswith('metric_')]\n",
    "input_columns = [\n",
    "    'blocksize', \n",
    "    'num_jobs', \n",
    "    'queue_depth',\n",
    "    'operating_pci_speed_gts',  \n",
    "    'operating_pci_width'\n",
    "] + metric_columns\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Step 1: Data Scaling using MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[input_columns])\n",
    "\n",
    "# Step 2: Outlier Detection and Removal using IQR\n",
    "Q1 = np.percentile(df_scaled, 25, axis=0)\n",
    "Q3 = np.percentile(df_scaled, 75, axis=0)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier thresholds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify and remove outliers\n",
    "outlier_indices = []\n",
    "for i in range(df_scaled.shape[1]):\n",
    "    outlier_list_col = df.index[(df_scaled[:, i] < lower_bound[i]) | (df_scaled[:, i] > upper_bound[i])].tolist()\n",
    "    outlier_indices.extend(outlier_list_col)\n",
    "\n",
    "outlier_indices = list(set(outlier_indices))  # Remove duplicates\n",
    "df_no_outliers = df.drop(index=outlier_indices).reset_index(drop=True)\n",
    "df_scaled_no_outliers = np.delete(df_scaled, outlier_indices, axis=0)\n",
    "print(f\"Number of outliers removed: {len(outlier_indices)}\")\n",
    "\n",
    "# Step 3: Apply PCA to reduce dimensionality while retaining 95% of variance\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "df_pca_no_outliers = pca.fit_transform(df_scaled_no_outliers)\n",
    "\n",
    "print(\"Variance explained by each PCA component:\", pca.explained_variance_ratio_)\n",
    "print(f\"Original number of features: {df_scaled_no_outliers.shape[1]}\")\n",
    "print(f\"Reduced number of features after PCA: {df_pca_no_outliers.shape[1]}\")\n",
    "\n",
    "# Step 4: Run KMeans clustering with 6 clusters\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    init='k-means++',\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "df_no_outliers['Cluster'] = kmeans.fit_predict(df_pca_no_outliers)\n",
    "\n",
    "# Step 5: Evaluate Clustering\n",
    "silhouette_avg = silhouette_score(df_pca_no_outliers, df_no_outliers['Cluster'])\n",
    "ch_score = calinski_harabasz_score(df_pca_no_outliers, df_no_outliers['Cluster'])\n",
    "db_score = davies_bouldin_score(df_pca_no_outliers, df_no_outliers['Cluster'])\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_score}\")\n",
    "print(f\"Davies-Bouldin Index: {db_score}\")\n",
    "\n",
    "\n",
    "cluster_mean_measure = df_no_outliers.groupby('Cluster')['mean_measure'].mean()\n",
    "print(\"Mean of 'mean_measure' for each cluster:\")\n",
    "print(cluster_mean_measure)\n",
    "\n",
    "# Step 6: Analyze Cluster Characteristics\n",
    "cluster_numerical_stats = df_no_outliers.groupby('Cluster')[input_columns].mean()\n",
    "print(\"Average Numerical Stats for Each Cluster:\")\n",
    "print(cluster_numerical_stats)\n",
    "\n",
    "# Step 7: Visualize Clusters using PCA Components\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(df_pca_no_outliers[:, 0], df_pca_no_outliers[:, 1], c=df_no_outliers['Cluster'], cmap='tab10', s=50)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title(\"KMeans Clusters Visualized with PCA Components\")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Visualize Clusters using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=500, random_state=42)\n",
    "df_tsne_no_outliers = tsne.fit_transform(df_pca_no_outliers)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "tsne_scatter = plt.scatter(df_tsne_no_outliers[:, 0], df_tsne_no_outliers[:, 1], c=df_no_outliers['Cluster'], cmap='tab10', s=50)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title(\"t-SNE Visualization of KMeans Clusters\")\n",
    "plt.colorbar(tsne_scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Generate a Heatmap of Mean Values for Each Feature Across Clusters\n",
    "cluster_means = df_no_outliers.groupby('Cluster')[input_columns].mean()\n",
    "cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cluster_means_normalized.T, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True, linewidths=0.5)\n",
    "plt.title('Normalized Mean Values of Input Features Across Clusters')\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral  Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assuming `df` is already loaded with the relevant data\n",
    "# Define numeric columns for clustering, including metric columns if available\n",
    "metric_columns = [col for col in df.columns if col.startswith('metric_')]\n",
    "numeric_columns = ['blocksize', 'num_jobs', 'queue_depth', 'operating_pci_speed_gts', 'operating_pci_width'] + metric_columns\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Apply PCA to reduce to 2 components for visualization\n",
    "pca = PCA(n_components=3)\n",
    "X_principal = pca.fit_transform(scaled_features)\n",
    "X_principal = pd.DataFrame(X_principal)\n",
    "X_principal.columns = ['P1', 'P2', 'P3']\n",
    "\n",
    "X_principal = pd.DataFrame(X_principal)\n",
    "\n",
    "display(X_principal)\n",
    "\n",
    "# Building the clustering model\n",
    "spectral_model = SpectralClustering(n_clusters=4, affinity='rbf')\n",
    " \n",
    "# Training the model and Storing the predicted cluster labels\n",
    "labels = spectral_model.fit_predict(X_principal)\n",
    "\n",
    "# Visualizing the clustering\n",
    "plt.scatter(X_principal['P1'], X_principal['P2'],\n",
    "            c=SpectralClustering(n_clusters=4, affinity='rbf') .fit_predict(X_principal), cmap=plt.cm.Set1)\n",
    "pt.title(\"Spectral clustering\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['Cluster'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
