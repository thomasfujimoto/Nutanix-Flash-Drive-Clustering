{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMean Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependences you'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary scikit-learn\n",
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define the columns to fetch from the database\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', \n",
    "    'blocksize', 'unit', 'min_measure', 'mean_measure', \n",
    "    'median_measure', 'max_measure', 'stddev_measure', 'device_type', \n",
    "    'family', 'vendor', 'model', 'firmware', 'capacity_GiB', \n",
    "    'operating_pci_speed_GTs', 'operating_pci_width', 'linkrate_Gbs', \n",
    "    'name', 'reference', 'created'\n",
    "]\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=False)\n",
    "\n",
    "# Check the columns in the DataFrame\n",
    "print(\"Available columns in DataFrame:\", df.columns.tolist())\n",
    "\n",
    "# Step 2: Define the features to use for clustering\n",
    "features = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', \n",
    "    'blocksize', 'unit', 'min_measure', 'mean_measure', \n",
    "    'median_measure', 'max_measure', 'stddev_measure', 'device_type', \n",
    "    'family', 'vendor', 'model', 'firmware', 'capacity_GiB', \n",
    "    'operating_pci_speed_GTs', 'operating_pci_width', 'linkrate_Gbs', \n",
    "    'name', 'reference', 'created'\n",
    "]\n",
    "\n",
    "# Step 3: Filter features to only include those present in the DataFrame\n",
    "features = [feature for feature in features if feature in df.columns]\n",
    "print(\"Using the following features for clustering:\", features)\n",
    "\n",
    "# Define categorical and numerical features based on available columns\n",
    "categorical_features = ['data_type', 'metric', 'unit', 'device_type', 'family', 'vendor', 'model', 'firmware', 'name', 'reference']\n",
    "numerical_features = ['queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', \n",
    "                     'median_measure', 'max_measure', 'stddev_measure', 'capacity_GiB', \n",
    "                     'operating_pci_speed_GTs', 'operating_pci_width', 'linkrate_Gbs']\n",
    "\n",
    "# Filter categorical and numerical features to only include those present in the DataFrame\n",
    "categorical_features = [feature for feature in categorical_features if feature in df.columns]\n",
    "numerical_features = [feature for feature in numerical_features if feature in df.columns]\n",
    "\n",
    "# Define the imputer for numerical features\n",
    "numerical_imputer = SimpleImputer(strategy='mean')  # You can choose other strategies like 'median' or 'most_frequent'\n",
    "\n",
    "# Preprocessing pipeline with imputer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', numerical_imputer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        ('cat', OneHotEncoder(sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 4: Clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)  # You can choose the number of clusters\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             ('clusterer', kmeans)])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(df[features])\n",
    "\n",
    "# Step 5: Evaluation\n",
    "df['cluster'] = pipeline.predict(df[features])\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(pipeline.named_steps['preprocessor'].transform(df[features]), df['cluster'])\n",
    "print(f'Silhouette Score: {silhouette_avg:.2f}')\n",
    "\n",
    "# Visualize the clusters\n",
    "# Step 6: Dimensionality Reduction for Visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(pipeline.named_steps['preprocessor'].transform(df[features]))\n",
    "\n",
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2'])\n",
    "pca_df['cluster'] = df['cluster']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=pca_df, x='PCA1', y='PCA2', hue='cluster', palette='viridis', s=100, alpha=0.7)\n",
    "plt.title('KMeans Clustering Visualization (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Check cluster assignments\n",
    "print(df[['cluster'] + features].head())\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'device_type', \n",
    "    'family', 'vendor', 'model', 'firmware', 'capacity_GiB', 'operating_pci_speed_GTs', \n",
    "    'operating_pci_width', 'linkrate_Gbs', 'name', 'reference', 'created'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=100, encode=True)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Check the encoding map if any encoding was applied and print the DataFrame\n",
    "print(\"Encoding Map:\", handler.encoding_map)\n",
    "print(df.head())\n",
    "\n",
    "# Data Preprocessing: Select numeric columns and fill missing values\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure'\n",
    "]\n",
    "\n",
    "# Impute missing values in numeric columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Determine the optimal number of clusters with the elbow method\n",
    "inertia = []\n",
    "k_values = range(1, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(df_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(k_values, inertia, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n",
    "\n",
    "# Apply KMeans with 6 clusters and calculate silhouette score\n",
    "kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "df['Cluster'] = kmeans.fit_predict(df_scaled)\n",
    "silhouette_avg = silhouette_score(df_scaled, kmeans.labels_)\n",
    "print(f\"Silhouette Score for 6 Clusters: {silhouette_avg}\")\n",
    "\n",
    "# Use PCA for 2D visualization of clusters\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df['Cluster'], cmap='viridis', s=50)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('KMeans Clusters with 2D PCA')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(df['Cluster']), title=\"Clusters\")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Display cluster summary by data_type\n",
    "cluster_summary = df.groupby(['data_type', 'Cluster']).size()\n",
    "print(\"Cluster Summary by Data Type:\\n\", cluster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'operating_pci_speed_gts', 'operating_pci_width'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=100000, encode=True)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Check encoding map and data preview\n",
    "print(\"Encoding Map:\", handler.encoding_map)\n",
    "print(df.head())\n",
    "\n",
    "# Define numeric columns for clustering\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure', 'capacity_gib', 'operating_pci_speed_gts', \n",
    "    'operating_pci_width'\n",
    "]\n",
    "\n",
    "# Impute missing values in numeric columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "# Initialize empty list to collect clusters\n",
    "all_clusters = []\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Iterate over each data type and apply KMeans clustering\n",
    "data_types = df['data_type'].unique()\n",
    "for data_type in data_types:\n",
    "    # Filter dataset by data type\n",
    "    df_filtered = df[df['data_type'] == data_type].copy()\n",
    "    \n",
    "    # Apply KMeans for 6 clusters\n",
    "    kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "    df_filtered['Cluster'] = kmeans.fit_predict(scaler.transform(df_filtered[numeric_columns]))\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(scaler.transform(df_filtered[numeric_columns]), df_filtered['Cluster'])\n",
    "    print(f\"Silhouette Score for {data_type} with 6 Clusters: {silhouette_avg}\")\n",
    "    \n",
    "    # Append filtered DataFrame with clusters to main DataFrame\n",
    "    all_clusters.append(df_filtered)\n",
    "\n",
    "    # Use PCA for 2D visualization of clusters\n",
    "    pca = PCA(n_components=2)\n",
    "    df_pca = pca.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "\n",
    "    # Plot the clusters for the current data type\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title(f'KMeans Clusters for {data_type} (6 clusters)')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "# Concatenate all clusters into the main DataFrame\n",
    "df_clusters = pd.concat(all_clusters)\n",
    "\n",
    "# Analyze cluster distribution by data type\n",
    "cluster_summary = df_clusters.groupby(['data_type', 'Cluster']).size()\n",
    "print(\"Cluster Summary by Data Type:\\n\", cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'operating_pci_speed_gts', 'operating_pci_width', 'device_type', 'model'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=True)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Define numeric columns for clustering\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure', 'capacity_gib', 'operating_pci_speed_gts', \n",
    "    'operating_pci_width'\n",
    "]\n",
    "\n",
    "# Impute missing values in numeric columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Initialize empty list to collect clusters\n",
    "all_clusters = []\n",
    "\n",
    "# Iterate over each data type and apply KMeans clustering\n",
    "data_types = df['data_type'].unique()\n",
    "print(\"Data Types: \" + ' and '.join(handler.encoding_map['data_type'][i] for i in data_types))\n",
    "for data_type in data_types:\n",
    "    # Filter dataset by data type\n",
    "    df_filtered = df[df['data_type'] == data_type].copy()\n",
    "    \n",
    "    # Apply KMeans for 6 clusters\n",
    "    kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "    df_filtered['Cluster'] = kmeans.fit_predict(scaler.transform(df_filtered[numeric_columns]))\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(scaler.transform(df_filtered[numeric_columns]), df_filtered['Cluster'])\n",
    "    print(f\"Silhouette Score for {handler.encoding_map['data_type'][data_type]} with 6 Clusters: {silhouette_avg}\")\n",
    "    \n",
    "    # Identify the highest performance cluster by mean of 'mean_measure' metric\n",
    "    cluster_performance = df_filtered.groupby('Cluster')['mean_measure'].mean()\n",
    "    top_cluster = cluster_performance.idxmax()\n",
    "    print(f\"Top Cluster for {handler.encoding_map['data_type'][data_type]}: Cluster {top_cluster}\")\n",
    "    \n",
    "    # Filter top cluster data\n",
    "    top_cluster_data = df_filtered[df_filtered['Cluster'] == top_cluster]\n",
    "    \n",
    "    # Find top-performing device type and model within the top cluster\n",
    "    top_device = top_cluster_data.groupby(['device_type', 'model'])['mean_measure'].mean().idxmax()\n",
    "    print(f\"Top-performing Device and Model for {handler.encoding_map['data_type'][data_type]}: {top_device}\")\n",
    "\n",
    "    # Append filtered DataFrame with clusters to main DataFrame\n",
    "    all_clusters.append(df_filtered)\n",
    "\n",
    "    # Use PCA for 2D visualization of clusters\n",
    "    pca = PCA(n_components=2)\n",
    "    df_pca = pca.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "\n",
    "    # Plot the clusters for the current data type\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title(f'KMeans Clusters for {handler.encoding_map['data_type'][data_type]} (6 clusters)')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "# Concatenate all clusters into the main DataFrame\n",
    "df_clusters = pd.concat(all_clusters)\n",
    "\n",
    "# Analyze cluster distribution by data type\n",
    "cluster_summary = df_clusters.groupby(['data_type', 'Cluster']).size()\n",
    "print(\"Cluster Summary by Data Type:\\n\", cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe - Take a look at this @Thomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'operating_pci_speed_gts', 'operating_pci_width', 'device_type', 'model'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=True)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Define numeric columns for clustering\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure', 'capacity_gib', 'operating_pci_speed_gts', \n",
    "    'operating_pci_width'\n",
    "]\n",
    "\n",
    "# Impute missing values in numeric columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Initialize empty list to collect clusters\n",
    "all_clusters = []\n",
    "\n",
    "# Iterate over each data type and metric and apply KMeans clustering\n",
    "data_types = df['data_type'].unique()\n",
    "metrics = df['metric'].unique()\n",
    "\n",
    "print(\"Data Types: \" + ' and '.join(handler.encoding_map['data_type'][i] for i in data_types))\n",
    "print(\"Metrics: \" + ' and '.join(handler.encoding_map['metric'][i] for i in metrics))\n",
    "\n",
    "for data_type in data_types:\n",
    "    for metric in metrics:\n",
    "        # Filter dataset by data type and metric\n",
    "        df_filtered = df[(df['data_type'] == data_type) & (df['metric'] == metric)].copy()\n",
    "        \n",
    "        # Skip if no data for this combination\n",
    "        if df_filtered.empty:\n",
    "            continue\n",
    "\n",
    "        # Apply KMeans for 6 clusters\n",
    "        kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "        df_filtered['Cluster'] = kmeans.fit_predict(scaler.transform(df_filtered[numeric_columns]))\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(scaler.transform(df_filtered[numeric_columns]), df_filtered['Cluster'])\n",
    "        print(f\"Silhouette Score for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]} with 6 Clusters: {silhouette_avg}\")\n",
    "        \n",
    "        # Identify the highest performance cluster by mean of 'mean_measure' metric\n",
    "        cluster_performance = df_filtered.groupby('Cluster')['mean_measure'].mean()\n",
    "        top_cluster = cluster_performance.idxmax()\n",
    "        print(f\"Top Cluster for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]}: Cluster {top_cluster}\")\n",
    "        \n",
    "        # Filter top cluster data\n",
    "        top_cluster_data = df_filtered[df_filtered['Cluster'] == top_cluster]\n",
    "        \n",
    "        # Find top-performing device type and model within the top cluster\n",
    "        top_device = top_cluster_data.groupby(['device_type', 'model'])['mean_measure'].mean().idxmax()\n",
    "        print(f\"Top-performing Device and Model for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]}: {top_device}\")\n",
    "\n",
    "        # Append filtered DataFrame with clusters to main DataFrame\n",
    "        all_clusters.append(df_filtered)\n",
    "\n",
    "        # Use PCA for 2D visualization of clusters\n",
    "        pca = PCA(n_components=2)\n",
    "        df_pca = pca.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "\n",
    "        # Plot the clusters for the current data type and metric\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.title(f'KMeans Clusters for {handler.encoding_map[\"data_type\"][data_type]} - {handler.encoding_map[\"metric\"][metric]} (6 clusters)')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.show()\n",
    "\n",
    "# Concatenate all clusters into the main DataFrame\n",
    "df_clusters = pd.concat(all_clusters)\n",
    "\n",
    "# Analyze cluster distribution by data type and metric\n",
    "cluster_summary = df_clusters.groupby(['data_type', 'metric', 'Cluster']).size()\n",
    "print(\"Cluster Summary by Data Type and Metric:\\n\", cluster_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
