{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'name', 'metric', 'queue_depth', 'num_jobs', 'blocksize','unit', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'device_type', 'model', 'operating_pci_speed_gts', 'operating_pci_width', \n",
    "]\n",
    "\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=False)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "# Filter dataset for Sequential Write\n",
    "\n",
    "# Define numeric columns for clustering (REMOVED:  'stddev_measure', 'median_measure','min_measure','max_measure', )\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure','operating_pci_speed_gts', 'operating_pci_width', \n",
    "]\n",
    "\n",
    "df = pd.get_dummies(df, columns=['metric'])\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Define data types to loop through\n",
    "data_types = [\"Random Read\", \"Random Write\", \"Sequential Write\", \"Sequential Read\"]\n",
    "\n",
    "# Loop through each data type and apply the clustering analysis\n",
    "for data_type in data_types:\n",
    "    print(f\"\\nProcessing data type: {data_type}\")\n",
    "\n",
    "    # Filter the DataFrame for the current data type\n",
    "    df_filtered = df[df['data_type'] == data_type].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Define input columns for clustering\n",
    "    input_columns = [\n",
    "        'blocksize', \n",
    "        'num_jobs', \n",
    "        'queue_depth', \n",
    "        # multiply num jub and queue depth (effective queue depth)\n",
    "        'operating_pci_speed_gts',  \n",
    "        'operating_pci_width'\n",
    "    ] \n",
    "    \n",
    "    print(input_columns)\n",
    "\n",
    "    # Step 1: Data Scaling using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_filtered[input_columns])\n",
    "\n",
    "    # Step 2: Outlier Detection and Removal using IQR\n",
    "    Q1 = np.percentile(df_scaled, 25, axis=0)\n",
    "    Q3 = np.percentile(df_scaled, 75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define outlier thresholds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify and remove outliers\n",
    "    outlier_indices = []\n",
    "    for i in range(df_scaled.shape[1]):\n",
    "        outlier_list_col = df_filtered.index[(df_scaled[:, i] < lower_bound[i]) | (df_scaled[:, i] > upper_bound[i])].tolist()\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "\n",
    "    outlier_indices = list(set(outlier_indices))  # Remove duplicates\n",
    "    df_no_outliers = df_filtered.drop(index=outlier_indices).reset_index(drop=True)\n",
    "    df_scaled_no_outliers = np.delete(df_scaled, outlier_indices, axis=0)\n",
    "    print(f\"Number of outliers removed for {data_type}: {len(outlier_indices)}\")\n",
    "\n",
    "    # Step 3: Apply PCA to reduce dimensionality while retaining 95% of variance\n",
    "    pca = PCA(n_components=0.95, random_state=42)\n",
    "    df_pca_no_outliers = pca.fit_transform(df_scaled_no_outliers)\n",
    "\n",
    "    print(\"Variance explained by each PCA component:\", pca.explained_variance_ratio_)\n",
    "    print(f\"Original number of features: {df_scaled_no_outliers.shape[1]}\")\n",
    "    print(f\"Reduced number of features after PCA: {df_pca_no_outliers.shape[1]}\")\n",
    "\n",
    "    # Step 4: Run KMeans clustering with 6 clusters\n",
    "    n_clusters = 8\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        init='k-means++',\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "    df_no_outliers['Cluster'] = kmeans.fit_predict(df_pca_no_outliers)\n",
    "\n",
    "    # Step 5: Evaluate Clustering\n",
    "    silhouette_avg = silhouette_score(df_pca_no_outliers, df_no_outliers['Cluster'])\n",
    "    ch_score = calinski_harabasz_score(df_pca_no_outliers, df_no_outliers['Cluster'])\n",
    "    db_score = davies_bouldin_score(df_pca_no_outliers, df_no_outliers['Cluster'])\n",
    "\n",
    "    print(f\"Silhouette Score for {data_type}: {silhouette_avg}\")\n",
    "    print(f\"Calinski-Harabasz Index for {data_type}: {ch_score}\")\n",
    "    print(f\"Davies-Bouldin Index for {data_type}: {db_score}\")\n",
    "\n",
    "    # Step 6: Analyze Cluster Characteristics\n",
    "    \n",
    "    cluster_counts = df_no_outliers['Cluster'].value_counts()\n",
    "    print(f\"\\nNumber of items in each cluster for {data_type}:\")\n",
    "    print(cluster_counts)\n",
    "\n",
    "    # Calculate the mean, standard deviation, and median of 'mean_measure' for each cluster\n",
    "    cluster_stats = df_no_outliers.groupby('Cluster')['mean_measure'].agg(['mean', 'std', 'median'])\n",
    "    print(df_no_outliers)\n",
    "    print(f\"Mean, Standard Deviation, and Median of 'mean_measure' for each cluster in {data_type}:\")\n",
    "    print(cluster_stats)\n",
    "\n",
    "    # Calculate average numerical statistics for each cluster\n",
    "    cluster_numerical_stats = df_no_outliers.groupby('Cluster')[input_columns].mean()\n",
    "    print(f\"Average Numerical Stats for Each Cluster in {data_type}:\")\n",
    "    print(cluster_numerical_stats)\n",
    "\n",
    "    # Step 7: Visualize Clusters using PCA Components\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(df_pca_no_outliers[:, 0], df_pca_no_outliers[:, 1], c=df_no_outliers['Cluster'], cmap='tab10', s=50)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title(f\"KMeans Clusters for {data_type} Visualized with PCA Components\")\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "    # Step 8: Visualize Clusters using t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=500, random_state=42)\n",
    "    df_tsne_no_outliers = tsne.fit_transform(df_pca_no_outliers)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    tsne_scatter = plt.scatter(df_tsne_no_outliers[:, 0], df_tsne_no_outliers[:, 1], c=df_no_outliers['Cluster'], cmap='tab10', s=50)\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.title(f\"t-SNE Visualization of KMeans Clusters for {data_type}\")\n",
    "    plt.colorbar(tsne_scatter, label='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "    # Step 9: Generate a Heatmap of Mean Values for Each Feature Across Clusters\n",
    "    cluster_means = df_no_outliers.groupby('Cluster')[input_columns].mean()\n",
    "    cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cluster_means_normalized.T, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True, linewidths=0.5)\n",
    "    plt.title(f'Normalized Mean Values of Input Features Across Clusters for {data_type}')\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
