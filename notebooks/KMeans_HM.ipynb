{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Heatmaps using Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'operating_pci_speed_gts', 'operating_pci_width', 'device_type', 'model'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=True)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Define numeric columns for clustering\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure', 'capacity_gib', 'operating_pci_speed_gts', \n",
    "    'operating_pci_width'\n",
    "]\n",
    "\n",
    "# Impute missing values in numeric columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Initialize empty list to collect clusters\n",
    "all_clusters = []\n",
    "\n",
    "# Iterate over each data type and metric and apply KMeans clustering\n",
    "data_types = df['data_type'].unique()\n",
    "metrics = df['metric'].unique()\n",
    "\n",
    "print(\"Data Types: \" + ' and '.join(handler.encoding_map['data_type'][i] for i in data_types))\n",
    "print(\"Metrics: \" + ' and '.join(handler.encoding_map['metric'][i] for i in metrics))\n",
    "\n",
    "for data_type in data_types:\n",
    "    for metric in metrics:\n",
    "        # Filter dataset by data type and metric\n",
    "        df_filtered = df[(df['data_type'] == data_type) & (df['metric'] == metric)].copy()\n",
    "        \n",
    "        # Skip if no data for this combination\n",
    "        if df_filtered.empty:\n",
    "            continue\n",
    "\n",
    "        # Apply KMeans for 6 clusters\n",
    "        kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "        df_filtered['Cluster'] = kmeans.fit_predict(scaler.transform(df_filtered[numeric_columns]))\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(scaler.transform(df_filtered[numeric_columns]), df_filtered['Cluster'])\n",
    "        print(f\"Silhouette Score for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]} with 6 Clusters: {silhouette_avg}\")\n",
    "        \n",
    "        # Identify the highest performance cluster by mean of 'mean_measure' metric\n",
    "        cluster_performance = df_filtered.groupby('Cluster')['mean_measure'].mean()\n",
    "        top_cluster = cluster_performance.idxmax()\n",
    "        print(f\"Top Cluster for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]}: Cluster {top_cluster}\")\n",
    "        \n",
    "        # Filter top cluster data\n",
    "        top_cluster_data = df_filtered[df_filtered['Cluster'] == top_cluster]\n",
    "        \n",
    "        # Find top-performing device type and model within the top cluster\n",
    "        top_device = top_cluster_data.groupby(['device_type', 'model'])['mean_measure'].mean().idxmax()\n",
    "        print(f\"Top-performing Device and Model for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]}: {top_device}\")\n",
    "\n",
    "        # Append filtered DataFrame with clusters to main DataFrame\n",
    "        all_clusters.append(df_filtered)\n",
    "\n",
    "        # Use PCA for 2D visualization of clusters\n",
    "        pca = PCA(n_components=2)\n",
    "        df_pca = pca.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "\n",
    "        # Plot the clusters for the current data type and metric\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.title(f'KMeans Clusters for {handler.encoding_map[\"data_type\"][data_type]} - {handler.encoding_map[\"metric\"][metric]} (6 clusters)')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.show()\n",
    "\n",
    "# Concatenate all clusters into the main DataFrame\n",
    "df_clusters = pd.concat(all_clusters)\n",
    "\n",
    "# Analyze cluster distribution by data type and metric\n",
    "cluster_summary = df_clusters.groupby(['data_type', 'metric', 'Cluster']).size()\n",
    "print(\"Cluster Summary by Data Type and Metric:\\n\", cluster_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
