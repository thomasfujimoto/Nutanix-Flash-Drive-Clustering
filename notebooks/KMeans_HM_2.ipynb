{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Heatmaps using Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEATMAPs w/ Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE  # Import t-SNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'operating_pci_speed_gts', 'operating_pci_width', 'device_type', 'model'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=True)\n",
    "\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Define numeric columns for clustering (REMOVED : 'queue_depth', 'num_jobs',)\n",
    "numeric_columns = [\n",
    "     'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure', 'operating_pci_speed_gts', \n",
    "    'operating_pci_width', 'capacity_gib' , 'operating_pci_speed_gts' \n",
    "]\n",
    "\n",
    "# Impute missing values in numeric columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Initialize empty list to collect clusters\n",
    "all_clusters = []\n",
    "\n",
    "# Iterate over each data type and metric and apply KMeans clustering\n",
    "data_types = df['data_type'].unique()\n",
    "metrics = df['metric'].unique()\n",
    "\n",
    "print(\"Data Types: \" + ' and '.join(handler.encoding_map['data_type'][i] for i in data_types))\n",
    "print(\"Metrics: \" + ' and '.join(handler.encoding_map['metric'][i] for i in metrics))\n",
    "\n",
    "for data_type in data_types:\n",
    "    for metric in metrics:\n",
    "        # Filter dataset by data type and metric\n",
    "        df_filtered = df[(df['data_type'] == data_type) & (df['metric'] == metric)].copy()\n",
    "        \n",
    "        # Skip if no data for this combination\n",
    "        if df_filtered.empty:\n",
    "            continue\n",
    "\n",
    "        # Apply KMeans for 6 clusters\n",
    "        kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "        df_filtered['Cluster'] = kmeans.fit_predict(scaler.transform(df_filtered[numeric_columns]))\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(scaler.transform(df_filtered[numeric_columns]), df_filtered['Cluster'])\n",
    "        print(f\"Silhouette Score for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]} with 6 Clusters: {silhouette_avg}\")\n",
    "        \n",
    "        # Identify the highest performance cluster by mean of 'mean_measure' metric\n",
    "        cluster_performance = df_filtered.groupby('Cluster')['mean_measure'].mean()\n",
    "        top_cluster = cluster_performance.idxmax()\n",
    "        print(f\"Top Cluster for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]}: Cluster {top_cluster}\")\n",
    "        \n",
    "        # Filter top cluster data\n",
    "        top_cluster_data = df_filtered[df_filtered['Cluster'] == top_cluster]\n",
    "        \n",
    "        # Find top-performing device type and model within the top cluster\n",
    "        top_device = top_cluster_data.groupby(['device_type', 'model'])['mean_measure'].mean().idxmax()\n",
    "        print(f\"Top-performing Device and Model for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]}: {top_device}\")\n",
    "\n",
    "        # Append filtered DataFrame with clusters to main DataFrame\n",
    "        all_clusters.append(df_filtered)\n",
    "\n",
    "        # Use PCA for 2D visualization of clusters\n",
    "        pca = PCA(n_components=2)\n",
    "        df_pca = pca.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "        \n",
    "        # Use Z Score to normalize data \n",
    "        cluster_means = df_filtered.groupby('Cluster')[numeric_columns].mean() # Calculate mean values for each cluster\n",
    "        cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std() # Find the z score normalization for each cluster and feature\n",
    "        cluster_means_normalized = cluster_means_normalized.T # Transpose for heatmap (clusters on x-axis, features on y-axis)\n",
    "\n",
    "        # Generate the heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cluster_means_normalized, annot=True, cmap=\"viridis\", fmt=\".2f\", cbar=True)\n",
    "        plt.title(f'Normalized Mean Values of Numeric Features Across Clusters for {handler.encoding_map[\"data_type\"][data_type]} - {handler.encoding_map[\"metric\"][metric]}')\n",
    "        plt.xlabel(\"Cluster\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the clusters for the current data type and metric\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.title(f'KMeans Clusters for {handler.encoding_map[\"data_type\"][data_type]} - {handler.encoding_map[\"metric\"][metric]} (6 clusters)')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot the clusters for the current data type and metric using t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=0, perplexity=30, n_iter=300)\n",
    "        df_tsne = tsne.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        tsne_scatter = plt.scatter(df_tsne[:, 0], df_tsne[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "        plt.xlabel('t-SNE Component 1')\n",
    "        plt.ylabel('t-SNE Component 2')\n",
    "        plt.title(f'KMeans Clusters for {handler.encoding_map[\"data_type\"][data_type]} - {handler.encoding_map[\"metric\"][metric]} (6 clusters) with t-SNE')\n",
    "        plt.colorbar(tsne_scatter, label='Cluster')\n",
    "        plt.show()\n",
    "\n",
    "# Concatenate all clusters into the main DataFrame\n",
    "df_clusters = pd.concat(all_clusters)\n",
    "\n",
    "# Analyze cluster distribution by data type and metric\n",
    "cluster_summary = df_clusters.groupby(['data_type', 'metric', 'Cluster']).size()\n",
    "print(\"Cluster Summary by Data Type and Metric:\\n\", cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEATMAPs w/ out Categorical Data & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'operating_pci_speed_gts', 'operating_pci_width', 'device_type', 'model'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=True)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Define numeric columns for clustering\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure', 'capacity_gib', 'operating_pci_speed_gts', \n",
    "    'operating_pci_width'\n",
    "]\n",
    "\n",
    "# Impute missing values in numeric columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Initialize empty list to collect clusters\n",
    "all_clusters = []\n",
    "\n",
    "# Iterate over each data type and apply KMeans clustering\n",
    "data_types = df['data_type'].unique()\n",
    "print(\"Data Types: \" + ' and '.join(handler.encoding_map['data_type'][i] for i in data_types))\n",
    "for data_type in data_types:\n",
    "    # Filter dataset by data type\n",
    "    df_filtered = df[df['data_type'] == data_type].copy()\n",
    "    \n",
    "    # Apply KMeans for 6 clusters\n",
    "    kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "    df_filtered['Cluster'] = kmeans.fit_predict(scaler.transform(df_filtered[numeric_columns]))\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(scaler.transform(df_filtered[numeric_columns]), df_filtered['Cluster'])\n",
    "    print(f\"Silhouette Score for {handler.encoding_map['data_type'][data_type]} with 6 Clusters: {silhouette_avg}\")\n",
    "    \n",
    "    # Identify the highest performance cluster by mean of 'mean_measure' metric\n",
    "    cluster_performance = df_filtered.groupby('Cluster')['mean_measure'].mean()\n",
    "    top_cluster = cluster_performance.idxmax()\n",
    "    print(f\"Top Cluster for {handler.encoding_map['data_type'][data_type]}: Cluster {top_cluster}\")\n",
    "    \n",
    "    # Filter top cluster data\n",
    "    top_cluster_data = df_filtered[df_filtered['Cluster'] == top_cluster]\n",
    "    \n",
    "    # Find top-performing device type and model within the top cluster\n",
    "    top_device = top_cluster_data.groupby(['device_type', 'model'])['mean_measure'].mean().idxmax()\n",
    "    print(f\"Top-performing Device and Model for {handler.encoding_map['data_type'][data_type]}: {top_device}\")\n",
    "\n",
    "    # Append filtered DataFrame with clusters to main DataFrame\n",
    "    all_clusters.append(df_filtered)\n",
    "\n",
    "    # Use PCA for 2D visualization of clusters\n",
    "    pca = PCA(n_components=2)\n",
    "    df_pca = pca.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "\n",
    "    # Plot the clusters for the current data type\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title(f\"KMeans Clusters for {handler.encoding_map['data_type'][data_type]} (6 clusters)\")\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.show()\n",
    "    \n",
    "    # Use Z Score to normalize data \n",
    "    cluster_means = df_filtered.groupby('Cluster')[numeric_columns].mean()  # Calculate mean values for each cluster\n",
    "    cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()  # Find the z score normalization for each cluster and feature\n",
    "    cluster_means_normalized = cluster_means_normalized.T  # Transpose for heatmap (clusters on x-axis, features on y-axis)\n",
    "\n",
    "    # Generate the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cluster_means_normalized, annot=True, cmap=\"viridis\", fmt=\".2f\", cbar=True)\n",
    "    plt.title(f'Normalized Mean Values of Numeric Features Across Clusters for {handler.encoding_map[\"data_type\"][data_type]}')\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the clusters for the current data type using t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=30, n_iter=300)\n",
    "    df_tsne = tsne.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    tsne_scatter = plt.scatter(df_tsne[:, 0], df_tsne[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.title(f'KMeans Clusters for {handler.encoding_map[\"data_type\"][data_type]} (6 clusters) with t-SNE')\n",
    "    plt.colorbar(tsne_scatter, label='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "# Concatenate all clusters into the main DataFrame\n",
    "df_clusters = pd.concat(all_clusters)\n",
    "\n",
    "# Analyze cluster distribution by data type\n",
    "cluster_summary = df_clusters.groupby(['data_type', 'Cluster']).size()\n",
    "print(\"Cluster Summary by Data Type:\\n\", cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEATMAPs w/ Categorical Data Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'operating_pci_speed_gts', 'operating_pci_width', 'device_type', 'model'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=True)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Define numeric columns for clustering\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure', 'capacity_gib', 'operating_pci_speed_gts', \n",
    "    'operating_pci_width'\n",
    "]\n",
    "\n",
    "# Impute missing values in numeric columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Initialize empty list to collect clusters\n",
    "all_clusters = []\n",
    "\n",
    "# Iterate over each data type and apply KMeans clustering\n",
    "data_types = df['data_type'].unique()\n",
    "print(\"Data Types: \" + ' and '.join(handler.encoding_map['data_type'][i] for i in data_types))\n",
    "for data_type in data_types:\n",
    "    # Filter dataset by data type\n",
    "    df_filtered = df[df['data_type'] == data_type].copy()\n",
    "    \n",
    "    # Apply KMeans for 6 clusters\n",
    "    kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "    df_filtered['Cluster'] = kmeans.fit_predict(scaler.transform(df_filtered[numeric_columns]))\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(scaler.transform(df_filtered[numeric_columns]), df_filtered['Cluster'])\n",
    "    print(f\"Silhouette Score for {handler.encoding_map['data_type'][data_type]} with 6 Clusters: {silhouette_avg}\")\n",
    "    \n",
    "    # Identify the highest performance cluster by mean of 'mean_measure' metric\n",
    "    cluster_performance = df_filtered.groupby('Cluster')['mean_measure'].mean()\n",
    "    top_cluster = cluster_performance.idxmax()\n",
    "    print(f\"Top Cluster for {handler.encoding_map['data_type'][data_type]}: Cluster {top_cluster}\")\n",
    "    \n",
    "    # Filter top cluster data\n",
    "    top_cluster_data = df_filtered[df_filtered['Cluster'] == top_cluster]\n",
    "    \n",
    "    # Find top-performing device type and model within the top cluster\n",
    "    top_device = top_cluster_data.groupby(['device_type', 'model'])['mean_measure'].mean().idxmax()\n",
    "    print(f\"Top-performing Device and Model for {handler.encoding_map['data_type'][data_type]}: {top_device}\")\n",
    "\n",
    "    # Append filtered DataFrame with clusters to main DataFrame\n",
    "    all_clusters.append(df_filtered)\n",
    "\n",
    "    # Use PCA for 2D visualization of clusters\n",
    "    pca = PCA(n_components=2)\n",
    "    df_pca = pca.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "\n",
    "    # Plot the clusters for the current data type\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title(f\"KMeans Clusters for {handler.encoding_map['data_type'][data_type]} (6 clusters)\")\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.show()\n",
    "    \n",
    "    # Use Z Score to normalize data \n",
    "    cluster_means = df_filtered.groupby('Cluster')[numeric_columns].mean()  # Calculate mean values for each cluster\n",
    "    cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()  # Find the z score normalization for each cluster and feature\n",
    "    cluster_means_normalized = cluster_means_normalized.T  # Transpose for heatmap (clusters on x-axis, features on y-axis)\n",
    "\n",
    "    # Generate the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cluster_means_normalized, annot=True, cmap=\"viridis\", fmt=\".2f\", cbar=True)\n",
    "    plt.title(f'Normalized Mean Values of Numeric Features Across Clusters for {handler.encoding_map[\"data_type\"][data_type]}')\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the clusters for the current data type using t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=30, n_iter=300)\n",
    "    df_tsne = tsne.fit_transform(scaler.transform(df_filtered[numeric_columns]))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    tsne_scatter = plt.scatter(df_tsne[:, 0], df_tsne[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.title(f'KMeans Clusters for {handler.encoding_map[\"data_type\"][data_type]} (6 clusters) with t-SNE')\n",
    "    plt.colorbar(tsne_scatter, label='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "# Concatenate all clusters into the main DataFrame\n",
    "df_clusters = pd.concat(all_clusters)\n",
    "\n",
    "# Analyze cluster distribution by data type\n",
    "cluster_summary = df_clusters.groupby(['data_type', 'Cluster']).size()\n",
    "print(\"Cluster Summary by Data Type:\\n\", cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # HEATMAPs w/ Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'operating_pci_speed_gts', 'operating_pci_width', 'device_type', 'model'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=True)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Define numeric and encoded categorical columns for clustering\n",
    "# Assuming 'data_type', 'device_type', 'model', etc., are already encoded in df\n",
    "all_columns_for_clustering = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure', 'capacity_gib', 'operating_pci_speed_gts', \n",
    "    'operating_pci_width', 'data_type', 'device_type', 'model'\n",
    "]\n",
    "\n",
    "# Impute missing values in the selected columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[all_columns_for_clustering] = imputer.fit_transform(df[all_columns_for_clustering])\n",
    "\n",
    "# Standardize the combined features (both numerical and encoded categorical)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[all_columns_for_clustering])\n",
    "\n",
    "# Initialize empty list to collect clusters\n",
    "all_clusters = []\n",
    "\n",
    "# Iterate over each data type and apply KMeans clustering\n",
    "data_types = df['data_type'].unique()\n",
    "print(\"Data Types: \" + ' and '.join(handler.encoding_map['data_type'][i] for i in data_types))\n",
    "for data_type in data_types:\n",
    "    # Filter dataset by data type\n",
    "    df_filtered = df[df['data_type'] == data_type].copy()\n",
    "    \n",
    "    # Apply KMeans for 6 clusters on combined data\n",
    "    kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "    df_filtered['Cluster'] = kmeans.fit_predict(scaler.transform(df_filtered[all_columns_for_clustering]))\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(scaler.transform(df_filtered[all_columns_for_clustering]), df_filtered['Cluster'])\n",
    "    print(f\"Silhouette Score for {handler.encoding_map['data_type'][data_type]} with 6 Clusters: {silhouette_avg}\")\n",
    "    \n",
    "    # Identify the highest performance cluster by mean of 'mean_measure' metric\n",
    "    cluster_performance = df_filtered.groupby('Cluster')['mean_measure'].mean()\n",
    "    top_cluster = cluster_performance.idxmax()\n",
    "    print(f\"Top Cluster for {handler.encoding_map['data_type'][data_type]}: Cluster {top_cluster}\")\n",
    "    \n",
    "    # Filter top cluster data\n",
    "    top_cluster_data = df_filtered[df_filtered['Cluster'] == top_cluster]\n",
    "    \n",
    "    # Find top-performing device type and model within the top cluster\n",
    "    top_device = top_cluster_data.groupby(['device_type', 'model'])['mean_measure'].mean().idxmax()\n",
    "    print(f\"Top-performing Device and Model for {handler.encoding_map['data_type'][data_type]}: {top_device}\")\n",
    "\n",
    "    # Append filtered DataFrame with clusters to main DataFrame\n",
    "    all_clusters.append(df_filtered)\n",
    "\n",
    "    # Use PCA for 2D visualization of clusters\n",
    "    pca = PCA(n_components=2)\n",
    "    df_pca = pca.fit_transform(scaler.transform(df_filtered[all_columns_for_clustering]))\n",
    "\n",
    "    # Plot the clusters for the current data type\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title(f\"KMeans Clusters for {handler.encoding_map['data_type'][data_type]} (6 clusters)\")\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.show()\n",
    "    \n",
    "    # Use Z Score to normalize data \n",
    "    cluster_means = df_filtered.groupby('Cluster')[all_columns_for_clustering].mean()  # Calculate mean values for each cluster\n",
    "    cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()  # Find the z score normalization for each cluster and feature\n",
    "    cluster_means_normalized = cluster_means_normalized.T  # Transpose for heatmap (clusters on x-axis, features on y-axis)\n",
    "\n",
    "    # Generate the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cluster_means_normalized, annot=True, cmap=\"viridis\", fmt=\".2f\", cbar=True)\n",
    "    plt.title(f'Normalized Mean Values of Features Across Clusters for {handler.encoding_map[\"data_type\"][data_type]}')\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the clusters for the current data type using t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=30, n_iter=300)\n",
    "    df_tsne = tsne.fit_transform(scaler.transform(df_filtered[all_columns_for_clustering]))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    tsne_scatter = plt.scatter(df_tsne[:, 0], df_tsne[:, 1], c=df_filtered['Cluster'], cmap='viridis', s=50)\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.title(f'KMeans Clusters for {handler.encoding_map[\"data_type\"][data_type]} (6 clusters) with t-SNE')\n",
    "    plt.colorbar(tsne_scatter, label='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "# Concatenate all clusters into the main DataFrame\n",
    "df_clusters = pd.concat(all_clusters)\n",
    "\n",
    "# Analyze cluster distribution by data type\n",
    "cluster_summary = df_clusters.groupby(['data_type', 'Cluster']).size()\n",
    "print(\"Cluster Summary by Data Type:\\n\", cluster_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
