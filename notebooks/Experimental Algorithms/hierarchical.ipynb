{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIERARCHICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data (Template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Connect to the database\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', \n",
    "    'blocksize', 'unit', 'min_measure', 'mean_measure', \n",
    "    'median_measure', 'max_measure', 'stddev_measure', 'device_type', \n",
    "    'family', 'vendor', 'model', 'firmware', 'capacity_GiB', \n",
    "    'operating_pci_speed_GTs', 'operating_pci_width', 'linkrate_Gbs', \n",
    "    'name', 'reference', 'created'\n",
    "]\n",
    "\n",
    "# Fetch data with encoding enabled\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=50000, encode=True)\n",
    "\n",
    "# Disconnect the handler\n",
    "handler.disconnect()\n",
    "\n",
    "# Display the DataFrame to verify encoded values\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the encoded data types and metrics\n",
    "data_types = [\n",
    "    {\"name\": \"Random Read Latency\", \"type\": 0, \"metric\": 2},\n",
    "    {\"name\": \"Random Write Latency\", \"type\": 1, \"metric\": 2},\n",
    "    {\"name\": \"Random Read\", \"type\": 0, \"metric\": None},\n",
    "    {\"name\": \"Random Write\", \"type\": 1, \"metric\": None},\n",
    "    {\"name\": \"Sequential Write\", \"type\": 3, \"metric\": None},\n",
    "    {\"name\": \"Sequential Read\", \"type\": 2, \"metric\": None},\n",
    "]\n",
    "\n",
    "# Loop through each data type and perform clustering\n",
    "for item in data_types:\n",
    "    name = item[\"name\"]\n",
    "    specific_type = item[\"type\"]\n",
    "    latency_metric = item[\"metric\"]\n",
    "    \n",
    "    print(f\"Processing {name}...\")\n",
    "\n",
    "    # Step 1: Filter the DataFrame for the specific data type\n",
    "    df_specific = df[df['data_type'] == specific_type].copy()\n",
    "    \n",
    "    # Step 2: If a latency metric is specified, filter for it as well\n",
    "    if latency_metric is not None:\n",
    "        df_specific = df_specific[df_specific['metric'] == latency_metric]\n",
    "\n",
    "    # Check if df_specific is empty\n",
    "    if df_specific.empty:\n",
    "        print(f\"No data found for data_type = '{name}'\")\n",
    "        continue\n",
    "\n",
    "    # Select numerical columns for clustering\n",
    "    df_numerical = df_specific.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Handle missing values by imputing\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df_numerical), columns=df_numerical.columns)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized = pd.DataFrame(scaler.fit_transform(df_imputed), columns=df_imputed.columns)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    num_features = df_normalized.shape[1]\n",
    "    pca = PCA(n_components=min(50, num_features))\n",
    "    df_pca = pd.DataFrame(pca.fit_transform(df_normalized))\n",
    "\n",
    "    # Perform hierarchical clustering\n",
    "    linkage_matrix = linkage(df_pca, method='ward')\n",
    "    distance_threshold = 20  # Adjust as needed\n",
    "    clusters = fcluster(linkage_matrix, t=distance_threshold, criterion='distance')\n",
    "    \n",
    "    # Add cluster labels back to the DataFrame\n",
    "    df_specific['Cluster'] = clusters\n",
    "\n",
    "    # Calculate silhouette score if more than one cluster exists\n",
    "    if len(set(clusters)) > 1:\n",
    "        silhouette_avg = silhouette_score(df_normalized, clusters)\n",
    "        print(f'Silhouette Score for {name}: {silhouette_avg:.4f}')\n",
    "    else:\n",
    "        print(f'Silhouette Score for {name}: Not applicable (only one cluster)')\n",
    "\n",
    "    # Visualize with dendrogram\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram(linkage_matrix, truncate_mode='level', p=5)\n",
    "    plt.title(f'Hierarchical Clustering Dendrogram for {name}')\n",
    "    plt.xlabel('Sample index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize with t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(df_pca)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=tsne_results[:, 0], y=tsne_results[:, 1], hue=clusters, palette='tab10', s=100, alpha=0.7)\n",
    "    plt.title(f't-SNE Visualization of Clusters for {name}')\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "    # Summarize clusters by selecting only numeric columns\n",
    "    numeric_columns = df_specific.select_dtypes(include=['float64', 'int64']).columns\n",
    "    cluster_summary = df_specific.groupby('Cluster')[numeric_columns].mean()\n",
    "    print(f\"Cluster Summary for {name}:\")\n",
    "    print(cluster_summary)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
