{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affinity Propogation\n",
    "\n",
    "Best use cases:\n",
    "\n",
    "Medium-sized datasets where the number of clusters is not known beforehand.\n",
    "\n",
    "\n",
    "Applications where flexibility in cluster size is needed.\n",
    "\n",
    "\n",
    "How does affinity propagation work?\n",
    "\n",
    "1. Determine similarity between different points based off of euclidean distance: S(i,k) ‎ =  -| x(i) - x(k) | ^2 \n",
    "2. Determine responsibility matrix\n",
    "    1. For every point, determine if this point would be a good exemplar compared to the other data points\n",
    "    2. Updates the likelihood that a point is the best exemplar\n",
    "3. Determine availability matrix\n",
    "    1. Ask yourself: when this data point chooses another data point as its exemplar, how good of a candidate is that point is as an exemplar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn seaborn matplotlib scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch, including 'data_type'\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', \n",
    "    'blocksize', 'unit', 'min_measure', 'mean_measure', \n",
    "    'median_measure', 'max_measure', 'stddev_measure', 'device_type', \n",
    "    'family', 'vendor', 'model', 'firmware', 'capacity_GiB', \n",
    "    'operating_pci_speed_GTs', 'operating_pci_width', 'linkrate_Gbs', \n",
    "    'name', 'reference', 'created'\n",
    "]\n",
    "\n",
    "# Not encoded\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=100000, encode=True)\n",
    "# Check the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Step 1: Select Numerical Columns\n",
    "df_numerical = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Step 2: Handle Missing Values by Imputing (Filling with Mean)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df_numerical), columns=df_numerical.columns)\n",
    "\n",
    "# Optional: Convert to float32 to save memory\n",
    "df_imputed = df_imputed.astype('float32')\n",
    "\n",
    "# Step 3: Normalize the Numerical Data Using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df_imputed), columns=df_imputed.columns)\n",
    "\n",
    "# Optional: Sample the Data to Reduce Computational Load\n",
    "# Note: Adjust 'n_samples' based on your system's capacity\n",
    "n_samples = 10000  # For example, 10,000 samples\n",
    "if len(df_normalized) > n_samples:\n",
    "    df_normalized = df_normalized.sample(n=n_samples, random_state=42)\n",
    "    df_original_sampled = df.iloc[df_normalized.index].copy()\n",
    "else:\n",
    "    df_original_sampled = df.copy()\n",
    "\n",
    "# # Check the DataFrame\n",
    "# print(df)\n",
    "handler.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define a List of Different Preference Values to Test\n",
    "preference_values = [-400,-350,-300]\n",
    "\n",
    "for preference in preference_values:\n",
    "    try:\n",
    "        print(f\"\\nProcessing Affinity Propagation with preference={preference}...\")\n",
    "\n",
    "        # Step 4: Apply Affinity Propagation for Clustering\n",
    "        clusterer = AffinityPropagation(\n",
    "            preference=preference,\n",
    "            damping=0.9,\n",
    "            max_iter=1000,\n",
    "            convergence_iter=100,\n",
    "            random_state=42\n",
    "        )\n",
    "        cluster_labels = clusterer.fit_predict(df_normalized)\n",
    "\n",
    "        # Add the Cluster Labels Back to the Original Sampled DataFrame\n",
    "        df_original_sampled['Cluster'] = cluster_labels\n",
    "\n",
    "        # Step 5: Apply t-SNE to Reduce to 2 Dimensions for Visualization\n",
    "        tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "        df_tsne = pd.DataFrame(tsne.fit_transform(df_normalized), columns=['t-SNE1', 't-SNE2'])\n",
    "        df_tsne['Cluster'] = cluster_labels\n",
    "\n",
    "        # Step 6: Plot the t-SNE-Transformed Data with Cluster Labels\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.scatterplot(\n",
    "            data=df_tsne,\n",
    "            x='t-SNE1',\n",
    "            y='t-SNE2',\n",
    "            hue='Cluster',\n",
    "            palette='tab10',\n",
    "            s=100,\n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.title(f't-SNE of Data Points (preference={preference})')\n",
    "        plt.xlabel('t-SNE Component 1')\n",
    "        plt.ylabel('t-SNE Component 2')\n",
    "        plt.legend(title='Cluster')\n",
    "        plt.show()\n",
    "\n",
    "        # Step 7: Analyze How the Original Features Differ by Cluster\n",
    "        numeric_columns = df_original_sampled.select_dtypes(include=['float64', 'int64']).columns\n",
    "        cluster_summary = df_original_sampled.groupby('Cluster')[numeric_columns].mean()\n",
    "\n",
    "        # Display the Summary Statistics by Cluster\n",
    "        print(f\"\\nCluster Summary Statistics for preference={preference}:\")\n",
    "        print(cluster_summary)\n",
    "\n",
    "        # Step 8: Visualize How 'queue_depth' Differs Across Clusters (If Exists)\n",
    "        if 'queue_depth' in df_original_sampled.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(x='Cluster', y='queue_depth', data=df_original_sampled)\n",
    "            plt.title(f'Distribution of Queue Depth by Cluster (preference={preference})')\n",
    "            plt.show()\n",
    "\n",
    "        # Step 9: Visualize How 'num_jobs' Differs Across Clusters (If Exists)\n",
    "        if 'num_jobs' in df_original_sampled.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(x='Cluster', y='num_jobs', data=df_original_sampled)\n",
    "            plt.title(f'Distribution of Number of Jobs by Cluster (preference={preference})')\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for preference={preference}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
