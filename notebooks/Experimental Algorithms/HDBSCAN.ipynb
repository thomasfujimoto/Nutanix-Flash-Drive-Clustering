{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data (Template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch, including 'data_type'\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', \n",
    "    'blocksize', 'unit', 'min_measure', 'mean_measure', \n",
    "    'median_measure', 'max_measure', 'stddev_measure', 'device_type', \n",
    "    'family', 'vendor', 'model', 'firmware', 'capacity_GiB', \n",
    "    'operating_pci_speed_GTs', 'operating_pci_width', 'linkrate_Gbs', \n",
    "    'name', 'reference', 'created'\n",
    "]\n",
    "\n",
    "# Not encoded\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=145000, encode=True)\n",
    "# Check the DataFrame\n",
    "print(df)\n",
    "\n",
    "handler.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define clustering parameters\n",
    "min_samples = 1000\n",
    "default_cluster_size = 1000\n",
    "latency_cluster_size = 500\n",
    "\n",
    "# Target features and special metric condition\n",
    "target_features = [\n",
    "    \"Sequential Write\", \"Sequential Read\", \n",
    "    \"Random Write\", \"Random Read\", \n",
    "    \"Random Write Latency\", \"Random Read Latency\"\n",
    "]\n",
    "latency_types = {\"Random Write Latency\", \"Random Read Latency\"}  # Only apply metric == 2 for these types\n",
    "\n",
    "# Obtain unique data_type values and map them to their names\n",
    "data_type_values = df['data_type'].unique()\n",
    "encoding_map = {value: handler.encoding_map['data_type'][value] for value in data_type_values}\n",
    "\n",
    "# Filter only for the relevant data types\n",
    "selected_data_types = [\n",
    "    {\"name\": encoding_map[i], \"encoded_value\": i} \n",
    "    for i in data_type_values if encoding_map.get(i) in target_features\n",
    "]\n",
    "\n",
    "# Process each selected data type\n",
    "for item in selected_data_types:\n",
    "    name = item[\"name\"]\n",
    "    specific_encoded_value = item[\"encoded_value\"]\n",
    "\n",
    "    print(f\"\\nProcessing {name}...\")\n",
    "\n",
    "    # Step 1: Filter DataFrame based on whether the type is latency-related\n",
    "    if name in latency_types:\n",
    "        # Apply metric == 2 filter for latency types\n",
    "        df_specific = df[(df['data_type'] == specific_encoded_value) & (df['metric'] == 2)].copy()\n",
    "        min_cluster_size = latency_cluster_size  # Use a smaller cluster size for latency types\n",
    "    else:\n",
    "        # No metric filter for non-latency types\n",
    "        df_specific = df[df['data_type'] == specific_encoded_value].copy()\n",
    "        min_cluster_size = default_cluster_size  # Use the default cluster size for non-latency types\n",
    "\n",
    "    # Check if df_specific is empty\n",
    "    if df_specific.empty:\n",
    "        print(f\"No data found for data_type = '{name}'\")\n",
    "        continue\n",
    "\n",
    "    # Step 2: Select numerical columns and handle missing values\n",
    "    df_numerical = df_specific.select_dtypes(include=['float64', 'int64'])\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df_numerical), columns=df_numerical.columns)\n",
    "\n",
    "    # Step 3: Normalize the numerical data\n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized = pd.DataFrame(scaler.fit_transform(df_imputed), columns=df_imputed.columns)\n",
    "\n",
    "    # Step 4: Apply HDBSCAN for clustering\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, cluster_selection_epsilon=0.1)\n",
    "    cluster_labels = clusterer.fit_predict(df_normalized)\n",
    "\n",
    "    # Add the cluster labels back to the filtered DataFrame\n",
    "    df_specific['Cluster'] = cluster_labels\n",
    "\n",
    "    # Calculate silhouette score if there is more than one cluster\n",
    "    non_noise_points = df_normalized[cluster_labels != -1]\n",
    "    non_noise_labels = cluster_labels[cluster_labels != -1]\n",
    "\n",
    "    if len(set(non_noise_labels)) > 1:\n",
    "        score = silhouette_score(non_noise_points, non_noise_labels)\n",
    "        print(f'Silhouette Score for {name} with min_samples={min_samples}: {score:.4f}')\n",
    "    else:\n",
    "        print(f'Silhouette Score for {name} with min_samples={min_samples}: Not applicable (only one cluster)')\n",
    "\n",
    "    # Additional HDBSCAN-specific metrics\n",
    "    stability_scores = clusterer.cluster_persistence_\n",
    "    print(f\"Cluster Stability Scores for {name}: {stability_scores}\")\n",
    "\n",
    "    outlier_scores = clusterer.outlier_scores_\n",
    "    df_specific['Outlier Score'] = outlier_scores\n",
    "    print(f\"Average Outlier Score for {name}: {outlier_scores.mean():.4f}\")\n",
    "\n",
    "    # Display summary information about clusters\n",
    "    cluster_summary = df_specific.groupby('Cluster').size()\n",
    "    print(f\"Cluster summary for {name}:\")\n",
    "    print(cluster_summary)\n",
    "\n",
    "    # Step 5: Apply t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(df_normalized)\n",
    "\n",
    "    # Plotting the clusters with t-SNE\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    unique_labels = set(cluster_labels)\n",
    "    for label in unique_labels:\n",
    "        label_mask = (cluster_labels == label)\n",
    "        plt.scatter(tsne_results[label_mask, 0], tsne_results[label_mask, 1], \n",
    "                    label=f'Cluster {label}' if label != -1 else 'Noise', s=10)\n",
    "    \n",
    "    plt.title(f\"t-SNE Visualization of HDBSCAN Clusters for {name}\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
