{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'name', 'metric', 'queue_depth', 'num_jobs', 'blocksize','unit', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'device_type', 'model', 'operating_pci_speed_gts', 'operating_pci_width', \n",
    "]\n",
    "\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=False)\n",
    "df = df[df['data_type'] == 'Sequential Write']\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "# Filter dataset for Sequential Write\n",
    "\n",
    "# Define numeric columns for clustering (REMOVED:  'stddev_measure', 'median_measure','min_measure','max_measure', )\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure','operating_pci_speed_gts', 'operating_pci_width', \n",
    "]\n",
    "\n",
    "df = pd.get_dummies(df, columns=['metric'])\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input columns for clustering, excluding 'mean_measure' and any other output metrics\n",
    "input_columns = [\n",
    "    'blocksize', \n",
    "    'num_jobs', \n",
    "    'queue_depth',\n",
    "    'operating_pci_speed_gts',  \n",
    "    'operating_pci_width', \n",
    "    # 'capacity_gib', \n",
    "    'metric_bw',\n",
    "    'metric_iops', \n",
    "    'metric_latency'\n",
    "]\n",
    "\n",
    "\n",
    "# # Impute missing values in input columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[input_columns] = imputer.fit_transform(df[input_columns])\n",
    "\n",
    "# Standardize input features for clustering\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[input_columns])\n",
    "\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=0.96)  # Retain 96% of the variance\n",
    "principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Apply KMeans for 6 clusters\n",
    "kmeans = kmeans = KMeans(\n",
    "    n_clusters=8,\n",
    "    init='k-means++',\n",
    "    n_init=100,\n",
    "    max_iter=5000,\n",
    "    tol=1e-4,\n",
    "    random_state=42,\n",
    "    algorithm='elkan',\n",
    "    verbose=0\n",
    ")\n",
    "df['Cluster'] = kmeans.fit_predict(df_scaled)\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(df_scaled, df['Cluster'])\n",
    "print(f\"Silhouette Score with 6 Clusters (excluding 'mean_measure' and other output values): {silhouette_avg}\")\n",
    "\n",
    "# Calculate mean of 'mean_measure' for each cluster\n",
    "cluster_mean_measure = df.groupby('Cluster')['mean_measure'].mean()\n",
    "\n",
    "# Sort clusters by mean_measure in descending order and display\n",
    "sorted_cluster_mean_measure = cluster_mean_measure.sort_values(ascending=False)\n",
    "print(\"Clusters ordered by 'mean_measure':\")\n",
    "print(sorted_cluster_mean_measure)\n",
    "\n",
    "\n",
    "# Identify the highest performance cluster based on mean of 'mean_measure'\n",
    "top_cluster = cluster_mean_measure.idxmax()\n",
    "print(f\"Top Cluster based on 'mean_measure': Tiering {top_cluster}\")\n",
    "\n",
    "# Use PCA for 2D visualization of clusters\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df['Cluster'], cmap='viridis', s=50)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title(\"KMeans Clusters (excluding 'mean_measure' and output values)\")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Generate a heatmap of the mean values for each input feature across clusters\n",
    "cluster_means = df.groupby('Cluster')[input_columns].mean()\n",
    "cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cluster_means_normalized.T, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True, linewidths=0.5)\n",
    "plt.title('Normalized Mean Values of Input Features Across Clusters')\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Define a range of cluster numbers to test\n",
    "cluster_range = range(2, 15)\n",
    "inertia_values = []\n",
    "silhouette_scores = []  # Initialize silhouette_scores\n",
    "\n",
    "# Define input columns for clustering, excluding 'mean_measure' and any other output metrics\n",
    "input_columns = [\n",
    "    'blocksize', \n",
    "    'num_jobs', \n",
    "    'queue_depth',\n",
    "    'operating_pci_speed_gts',  \n",
    "    'operating_pci_width', \n",
    "    # 'capacity_gib', \n",
    "    'metric_bw',\n",
    "    'metric_iops', \n",
    "    'metric_latency'\n",
    "]\n",
    "\n",
    "# Standardize input features for clustering\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[input_columns])\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=0.96)  # Retain 96% of the variance\n",
    "principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Loop over different numbers of clusters and compute inertia and silhouette score\n",
    "for n_clusters in cluster_range:\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        init='k-means++',\n",
    "        n_init=100,\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        algorithm='elkan'\n",
    "    )\n",
    "    cluster_labels = kmeans.fit_predict(principal_components)\n",
    "    # inertia_values.append(kmeans.inertia_)\n",
    "    \n",
    "    sil_score = silhouette_score(principal_components, cluster_labels)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    print(sil_score)\n",
    "\n",
    "# # Plot the Elbow Graph\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(cluster_range, inertia_values, marker='o', label='Inertia')\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Plot the Silhouette Scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o', color='red', label='Silhouette Score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis for Optimal Number of Clusters')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Define input columns for clustering, excluding 'mean_measure' and any other output metrics\n",
    "input_columns = [\n",
    "    'blocksize', \n",
    "    'num_jobs', \n",
    "    'queue_depth',\n",
    "    'operating_pci_speed_gts',  \n",
    "    'operating_pci_width', \n",
    "    # 'capacity_gib', \n",
    "    'metric_bw',\n",
    "    # 'metric_iops', \n",
    "    'metric_latency'\n",
    "]\n",
    "\n",
    "# Standardize input features for clustering\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df[input_columns])\n",
    "\n",
    "# Step 1: Detect and Remove Outliers Based on Z-Score\n",
    "z_scores = np.abs((df_scaled - np.mean(df_scaled, axis=0)) / np.std(df_scaled, axis=0))  # Calculate Z-scores\n",
    "outlier_threshold = 3  # Define Z-score threshold for outliers\n",
    "outlier_indices = np.where(z_scores > outlier_threshold)[0]  # Find indices of outliers\n",
    "\n",
    "# Remove outliers from the original dataframe and the scaled data\n",
    "df_no_outliers = df.drop(index=outlier_indices).reset_index(drop=True)\n",
    "df_scaled_no_outliers = np.delete(df_scaled, outlier_indices, axis=0)\n",
    "\n",
    "# Print the number of outliers that were dropped\n",
    "num_outliers = len(np.unique(outlier_indices))  # Get unique outlier indices and count them\n",
    "print(f\"Number of outliers removed: {num_outliers}\")\n",
    "\n",
    "# Step 2: Apply KMeans for clustering on the scaled, outlier-removed data\n",
    "kmeans = KMeans(\n",
    "    n_clusters=8,            \n",
    "    init='random',        \n",
    "    n_init=5000,             \n",
    "    max_iter=10000,          \n",
    "    tol=1e-4,                \n",
    "    random_state=42,         \n",
    "    algorithm='lloyd',       \n",
    "    verbose=0                \n",
    ")\n",
    "\n",
    "df_no_outliers['Cluster'] = kmeans.fit_predict(df_scaled_no_outliers)\n",
    "\n",
    "# Step 3: Calculate Silhouette Score\n",
    "silhouette_avg = silhouette_score(df_scaled_no_outliers, df_no_outliers['Cluster'])\n",
    "print(f\"Silhouette Score with 8 Clusters (excluding outliers): {silhouette_avg}\")\n",
    "\n",
    "# Step 4: Calculate Mean of 'mean_measure' for Each Cluster\n",
    "cluster_mean_measure = df_no_outliers.groupby('Cluster')['mean_measure'].mean()\n",
    "\n",
    "# Calculate average numerical statistics for each cluster\n",
    "numerical_columns = [\n",
    "    'blocksize', \n",
    "    'num_jobs', \n",
    "    'queue_depth',\n",
    "    'operating_pci_speed_gts',  \n",
    "    'operating_pci_width'\n",
    "]\n",
    "cluster_numerical_stats = df_no_outliers.groupby('Cluster')[numerical_columns].mean()\n",
    "\n",
    "# Print the average numerical statistics for each cluster\n",
    "print(\"Average Numerical Stats for Each Cluster:\")\n",
    "print(cluster_numerical_stats)\n",
    "\n",
    "# Dominance of 'metric_bw', 'metric_iops', 'metric_latency' in each cluster\n",
    "dominant_features = ['metric_bw', 'metric_iops', 'metric_latency']\n",
    "cluster_dominance_stats = df_no_outliers.groupby('Cluster')[dominant_features].agg(lambda x: x.mode()[0])\n",
    "\n",
    "print(\"\\nMost Dominant One-Hot Encoded Feature in Each Cluster:\")\n",
    "print(cluster_dominance_stats)\n",
    "\n",
    "# Sort clusters by 'mean_measure' in descending order and display\n",
    "sorted_cluster_mean_measure = cluster_mean_measure.sort_values(ascending=False)\n",
    "print(\"Clusters ordered by 'mean_measure':\")\n",
    "print(sorted_cluster_mean_measure)\n",
    "\n",
    "# Identify the highest performance cluster based on 'mean_measure'\n",
    "top_cluster = cluster_mean_measure.idxmax()\n",
    "print(f\"Top Cluster based on 'mean_measure': Tiering {top_cluster}\")\n",
    "\n",
    "# Step 5: Use PCA for 2D Visualization of Clusters\n",
    "pca = PCA(n_components=2)\n",
    "df_pca_no_outliers = pca.fit_transform(df_scaled_no_outliers)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(df_pca_no_outliers[:, 0], df_pca_no_outliers[:, 1], c=df_no_outliers['Cluster'], cmap='viridis', s=50)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title(\"KMeans Clusters (excluding outliers)\")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Generate a Heatmap of Mean Values for Each Feature Across Clusters\n",
    "cluster_means = df_no_outliers.groupby('Cluster')[input_columns].mean()\n",
    "cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cluster_means_normalized.T, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True, linewidths=0.5)\n",
    "plt.title('Normalized Mean Values of Input Features Across Clusters')\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Use t-SNE for Visualization of Clusters (optional)\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=42)\n",
    "df_tsne_no_outliers = tsne.fit_transform(df_scaled_no_outliers)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "tsne_scatter = plt.scatter(df_tsne_no_outliers[:, 0], df_tsne_no_outliers[:, 1], c=df_no_outliers['Cluster'], cmap='viridis', s=50)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title(\"t-SNE Visualization of KMeans Clusters (excluding outliers)\")\n",
    "plt.colorbar(tsne_scatter, label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied PCA (optimally reduce features) + Outliers + Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Assume 'df' is your DataFrame\n",
    "# If not already loaded, you should load your data into 'df' before running this code\n",
    "\n",
    "# Define input columns for clustering\n",
    "# Get all metric dummy columns\n",
    "metric_columns = [col for col in df.columns if col.startswith('metric_')]\n",
    "\n",
    "# Define input columns\n",
    "input_columns = [\n",
    "    'blocksize', \n",
    "    'num_jobs', \n",
    "    'queue_depth',\n",
    "    'operating_pci_speed_gts',  \n",
    "    'operating_pci_width'\n",
    "] + metric_columns\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Step 1: Data Scaling using MinMaxScaler to normalize data\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df[input_columns])\n",
    "\n",
    "# Step 2: Refine Outlier Detection using IQR method\n",
    "Q1 = np.percentile(df_scaled, 25, axis=0)\n",
    "Q3 = np.percentile(df_scaled, 75, axis=0)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier thresholds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outlier_indices = []\n",
    "for i in range(df_scaled.shape[1]):\n",
    "    outlier_list_col = df.index[(df_scaled[:, i] < lower_bound[i]) | (df_scaled[:, i] > upper_bound[i])].tolist()\n",
    "    outlier_indices.extend(outlier_list_col)\n",
    "\n",
    "# Remove duplicates\n",
    "outlier_indices = list(set(outlier_indices))\n",
    "\n",
    "# Remove outliers from the original dataframe and the scaled data\n",
    "df_no_outliers = df.drop(index=outlier_indices).reset_index(drop=True)\n",
    "df_scaled_no_outliers = np.delete(df_scaled, outlier_indices, axis=0)\n",
    "\n",
    "print(f\"Number of outliers removed: {len(outlier_indices)}\")\n",
    "\n",
    "# Step 3: Apply PCA to reduce dimensionality while retaining 95% of variance\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "df_pca_no_outliers = pca.fit_transform(df_scaled_no_outliers)\n",
    "\n",
    "print(f\"Original number of features: {df_scaled_no_outliers.shape[1]}\")\n",
    "print(f\"Reduced number of features after PCA: {df_pca_no_outliers.shape[1]}\")\n",
    "\n",
    "# Step 4: Optimize the number of clusters using Silhouette Analysis\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# To avoid heavy computation, limit the range of clusters\n",
    "range_n_clusters = list(range(2, 10))\n",
    "silhouette_avgs = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        init='k-means++',\n",
    "        n_init=10,  # Reduced n_init to lower computation\n",
    "        max_iter=300,  # Default value\n",
    "        random_state=42\n",
    "    )\n",
    "    cluster_labels = kmeans.fit_predict(df_pca_no_outliers)\n",
    "    silhouette_avg = silhouette_score(df_pca_no_outliers, cluster_labels)\n",
    "    silhouette_avgs.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range_n_clusters, silhouette_avgs, marker='o')\n",
    "plt.title('Silhouette Score for Various Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal number of clusters (the one with the highest silhouette score)\n",
    "optimal_k = range_n_clusters[np.argmax(silhouette_avgs)]\n",
    "print(f\"Optimal number of clusters based on silhouette score: {optimal_k}\")\n",
    "\n",
    "# Step 5: Apply KMeans clustering with the optimal number of clusters\n",
    "kmeans = KMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    init='k-means++',\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "df_no_outliers['Cluster'] = kmeans.fit_predict(df_pca_no_outliers)\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(df_pca_no_outliers, df_no_outliers['Cluster'])\n",
    "print(f\"Silhouette Score with {optimal_k} Clusters: {silhouette_avg}\")\n",
    "\n",
    "# Step 6: Evaluate Clustering with Additional Metrics\n",
    "ch_score = calinski_harabasz_score(df_pca_no_outliers, df_no_outliers['Cluster'])\n",
    "db_score = davies_bouldin_score(df_pca_no_outliers, df_no_outliers['Cluster'])\n",
    "\n",
    "print(f\"Calinski-Harabasz Index: {ch_score}\")\n",
    "print(f\"Davies-Bouldin Index: {db_score}\")\n",
    "\n",
    "# Step 7: Analyze Cluster Characteristics\n",
    "# Calculate Mean of 'mean_measure' for Each Cluster\n",
    "cluster_mean_measure = df_no_outliers.groupby('Cluster')['mean_measure'].mean()\n",
    "\n",
    "# Calculate average numerical statistics for each cluster\n",
    "numerical_columns = [\n",
    "    'blocksize', \n",
    "    'num_jobs', \n",
    "    'queue_depth',\n",
    "    'operating_pci_speed_gts',  \n",
    "    'operating_pci_width'\n",
    "]\n",
    "cluster_numerical_stats = df_no_outliers.groupby('Cluster')[numerical_columns].mean()\n",
    "\n",
    "# Print the average numerical statistics for each cluster\n",
    "print(\"Average Numerical Stats for Each Cluster:\")\n",
    "print(cluster_numerical_stats)\n",
    "\n",
    "# Dynamically get available metric columns\n",
    "metric_columns = [col for col in df_no_outliers.columns if col.startswith('metric_')]\n",
    "\n",
    "# Check if metric columns are available\n",
    "if metric_columns:\n",
    "    # Dominant features in each cluster\n",
    "    dominant_features = metric_columns\n",
    "    cluster_dominance_stats = df_no_outliers.groupby('Cluster')[dominant_features].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "    print(\"\\nMost Dominant Features in Each Cluster:\")\n",
    "    print(cluster_dominance_stats)\n",
    "else:\n",
    "    print(\"\\nNo metric columns available to calculate dominant features.\")\n",
    "\n",
    "# Step 8: Visualize Clusters using PCA Components\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(df_pca_no_outliers[:, 0], df_pca_no_outliers[:, 1], c=df_no_outliers['Cluster'], cmap='tab10', s=50)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title(\"KMeans Clusters Visualized with PCA Components\")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Visualize Clusters using t-SNE\n",
    "# Adjust t-SNE parameters to be less resource-intensive\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=500, random_state=42)\n",
    "df_tsne_no_outliers = tsne.fit_transform(df_pca_no_outliers)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "tsne_scatter = plt.scatter(df_tsne_no_outliers[:, 0], df_tsne_no_outliers[:, 1], c=df_no_outliers['Cluster'], cmap='tab10', s=50)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title(\"t-SNE Visualization of KMeans Clusters\")\n",
    "plt.colorbar(tsne_scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Generate a Heatmap of Mean Values for Each Feature Across Clusters\n",
    "cluster_means = df_no_outliers.groupby('Cluster')[input_columns].mean()\n",
    "cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cluster_means_normalized.T, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True, linewidths=0.5)\n",
    "plt.title('Normalized Mean Values of Input Features Across Clusters')\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save cluster assignments to a CSV file\n",
    "# df_no_outliers.to_csv('clustered_data.csv', index=False)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
