{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Heatmaps using Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEATMAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "from data_access.postgres_handler import PostgresHandler\n",
    "\n",
    "# Initialize the PostgresHandler\n",
    "handler = PostgresHandler(\n",
    "    database=\"nutanix\",\n",
    "    user=\"postgres\",\n",
    "    host='172.25.221.34',\n",
    "    password=\"Senna\",\n",
    "    port=1433\n",
    ")\n",
    "handler.connect()\n",
    "\n",
    "# Define columns to fetch\n",
    "columns = [\n",
    "    'concord_id', 'data_type', 'metric', 'queue_depth', 'num_jobs', 'blocksize', 'min_measure', \n",
    "    'mean_measure', 'median_measure', 'max_measure', 'stddev_measure', 'capacity_gib', \n",
    "    'operating_pci_speed_gts', 'operating_pci_width', 'device_type', 'model'\n",
    "]\n",
    "df = handler.get_data(\"ssd_clean_data\", columns, limit=None, encode=True)\n",
    "\n",
    "# Disconnect from the database\n",
    "handler.disconnect()\n",
    "\n",
    "# Define initial numeric columns for clustering\n",
    "numeric_columns = [\n",
    "    'queue_depth', 'num_jobs', 'blocksize', 'min_measure', 'mean_measure', 'median_measure', \n",
    "    'max_measure', 'stddev_measure', 'capacity_gib', 'operating_pci_speed_gts', \n",
    "    'operating_pci_width'\n",
    "]\n",
    "\n",
    "# Subsets of features to remove\n",
    "feature_combinations = [\n",
    "    numeric_columns,\n",
    "    [col for col in numeric_columns if col != 'queue_depth'],\n",
    "    [col for col in numeric_columns if col != 'num_jobs'],\n",
    "    [col for col in numeric_columns if col not in ['queue_depth', 'num_jobs']],\n",
    "    [col for col in numeric_columns if col != 'blocksize'],\n",
    "    [col for col in numeric_columns if col not in ['capacity_gib', 'operating_pci_speed_gts']],\n",
    "]\n",
    "\n",
    "# Initialize SimpleImputer for missing data\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Loop over each feature subset\n",
    "for idx, selected_columns in enumerate(feature_combinations, start=1):\n",
    "    print(f\"\\n\\n--- Feature Set {idx}: Using Columns {selected_columns} ---\\n\")\n",
    "    \n",
    "    # Impute and standardize selected features\n",
    "    df[selected_columns] = imputer.fit_transform(df[selected_columns])\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df[selected_columns])\n",
    "\n",
    "    # Initialize list to collect clusters\n",
    "    all_clusters = []\n",
    "    \n",
    "    # Iterate over each data type and metric to apply KMeans clustering\n",
    "    data_types = df['data_type'].unique()\n",
    "    metrics = df['metric'].unique()\n",
    "    \n",
    "    print(\"Data Types: \" + ' and '.join(handler.encoding_map['data_type'][i] for i in data_types))\n",
    "    print(\"Metrics: \" + ' and '.join(handler.encoding_map['metric'][i] for i in metrics))\n",
    "    \n",
    "    for data_type in data_types:\n",
    "        for metric in metrics:\n",
    "            # Filter dataset by data type and metric\n",
    "            df_filtered = df[(df['data_type'] == data_type) & (df['metric'] == metric)].copy()\n",
    "            \n",
    "            # Skip if no data for this combination\n",
    "            if df_filtered.empty:\n",
    "                continue\n",
    "\n",
    "            # Apply KMeans for 6 clusters\n",
    "            kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "            df_filtered['Cluster'] = kmeans.fit_predict(scaler.transform(df_filtered[selected_columns]))\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            silhouette_avg = silhouette_score(scaler.transform(df_filtered[selected_columns]), df_filtered['Cluster'])\n",
    "            print(f\"Silhouette Score for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]} with 6 Clusters: {silhouette_avg}\")\n",
    "            \n",
    "            # Identify the highest performance cluster by mean of 'mean_measure' metric\n",
    "            cluster_performance = df_filtered.groupby('Cluster')['mean_measure'].mean()\n",
    "            top_cluster = cluster_performance.idxmax()\n",
    "            print(f\"Top Cluster for {handler.encoding_map['data_type'][data_type]} - {handler.encoding_map['metric'][metric]}: Cluster {top_cluster}\")\n",
    "            \n",
    "            # Append filtered DataFrame with clusters to main DataFrame\n",
    "            all_clusters.append(df_filtered)\n",
    "\n",
    "            # Use PCA for 2D visualization of clusters\n",
    "            pca = PCA(n_components=2)\n",
    "            df_pca = pca.fit_transform(scaler.transform(df_filtered[selected_columns]))\n",
    "            \n",
    "            # Normalizing cluster means for heatmap\n",
    "            cluster_means = df_filtered.groupby('Cluster')[selected_columns].mean()\n",
    "            cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "            cluster_means_normalized = cluster_means_normalized.T  # Transpose for heatmap\n",
    "            \n",
    "            # Generate the heatmap\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(cluster_means_normalized, annot=True, cmap=\"viridis\", fmt=\".2f\", cbar=True)\n",
    "            plt.title(f'Normalized Mean Values Across Clusters (Feature Set {idx}) - {handler.encoding_map[\"data_type\"][data_type]} - {handler.encoding_map[\"metric\"][metric]}')\n",
    "            plt.xlabel(\"Cluster\")\n",
    "            plt.ylabel(\"Feature\")\n",
    "            plt.show()\n",
    "    \n",
    "    # Concatenate all clusters into a DataFrame and analyze\n",
    "    df_clusters = pd.concat(all_clusters)\n",
    "    cluster_summary = df_clusters.groupby(['data_type', 'metric', 'Cluster']).size()\n",
    "    print(f\"\\nCluster Summary by Data Type and Metric for Feature Set {idx}:\\n\", cluster_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
